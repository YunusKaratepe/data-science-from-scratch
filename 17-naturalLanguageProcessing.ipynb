{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram models\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import data_analysis_tools as da\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "html = requests.get(url).text\n",
    "\n",
    "sp = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "content = sp.find('div', 'entry-content')\n",
    "regex = r\"[\\w']+|[\\.]\"\n",
    "\n",
    "document = []\n",
    "\n",
    "for p in content('p'):\n",
    "    words = re.findall(regex, fix_unicode(p.text))\n",
    "    document.extend(words)\n",
    "\n",
    "# continue on pg 338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'your']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"this means 'data' and 'you' are the 2 words which follows 'government' in this article.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word pairs\n",
    "from collections import defaultdict\n",
    "bigrams = zip(document, document[1:])\n",
    "transitions = defaultdict(list)\n",
    "\n",
    "for prev, current in bigrams:\n",
    "    transitions[prev].append(current)\n",
    "\n",
    "print(transitions['government'])\n",
    "\"\"\"this means 'data' and 'you' are the 2 words which follows 'government' in this article.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Random Bullshit Sentences ->\n",
      "\n",
      "   - 01 each .\n",
      "\n",
      "   - Whether you have is try plotting it goes .\n",
      "\n",
      "   - Whether that person's identity using Google is one of a sense of the open source and enormous datasets which are several it's revolutionary CDDB realized that people and made yourself you simply ignore the classic for the data platforms or studying the job only costs 100 .\n",
      "\n",
      "   - We've all heard a large collection tools for an allure but it easier to parse the companies using Yahoo to understanding the Cassandra jobs and Python design a lot about data science .\n",
      "\n",
      "   - We're discussing here .\n"
     ]
    }
   ],
   "source": [
    "# generating sentences\n",
    "def generate_using_bigrams(transitions):\n",
    "    current = \".\" # this means the next word will start a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]\n",
    "        current = da.random.sample(next_word_candidates)\n",
    "        result.append(current)\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)\n",
    "\n",
    "print('5 Random Bullshit Sentences ->')\n",
    "print('\\n   -', generate_using_bigrams(transitions))\n",
    "print('\\n   -', generate_using_bigrams(transitions))\n",
    "print('\\n   -', generate_using_bigrams(transitions))\n",
    "print('\\n   -', generate_using_bigrams(transitions))\n",
    "print('\\n   -', generate_using_bigrams(transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = zip(document, document[1:], document[2:])\n",
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in trigrams:\n",
    "    if prev == \".\":\n",
    "        starts.append(current)\n",
    "    trigram_transitions[(prev, current)].append(next)\n",
    "\n",
    "trigram_transitions[('successful', 'businesses')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Random Sentences ->\n",
      "\n",
      "   - The result was a valuable data product that analyzed a huge mountain of data science .\n",
      "\n",
      "   - ly is generating and find out just how bad your data sort the distractions from the data were insufficient .\n",
      "\n",
      "   - If you've ever used iTunes to rip a CD you've taken advantage of this data would be useless if we couldn't store it .\n",
      "\n",
      "   - Scripting languages such as Perl and Python are essential .\n",
      "\n",
      "   - Data is indeed the new Intel Inside .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'They seem much more realistic compare to biagrams tho.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_using_trigrams(transitions, starts):\n",
    "\n",
    "    current = da.random.sample(starts)\n",
    "    prev = \".\"\n",
    "    result = [current]\n",
    "\n",
    "    while True:\n",
    "        next_word_candidates = transitions[(prev, current)]\n",
    "        next_word = da.random.sample(next_word_candidates)\n",
    "        \n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)\n",
    "\n",
    "print('5 Random Sentences ->')\n",
    "print('\\n   -', generate_using_trigrams(trigram_transitions, starts))\n",
    "print('\\n   -', generate_using_trigrams(trigram_transitions, starts))\n",
    "print('\\n   -', generate_using_trigrams(trigram_transitions, starts))\n",
    "print('\\n   -', generate_using_trigrams(trigram_transitions, starts))\n",
    "print('\\n   -', generate_using_trigrams(trigram_transitions, starts))\n",
    "\n",
    "\"\"\"They seem much more realistic compare to biagrams tho.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammars\n",
    "grammar = {\n",
    "    \"_S\": [\"_NP _VP\"],\n",
    "    \"_NP\": [\"_N\", \"_A _NP _P _A _N\"],\n",
    "    \"_VP\": [\"_V\", \"_V _NP\"],\n",
    "    \"_N\": [\"data science\", \"Python\", \"regression\"],\n",
    "    \"_A\": [\"big\", \"linear\", \"logistic\"],\n",
    "    \"_P\": [\"about\", \"near\"],\n",
    "    \"_V\": [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a sentence is created like this ->\n",
    "* ['_S']\n",
    "* ['_NP','_VP']\n",
    "* ['_N','_VP']\n",
    "* ['Python','_VP']\n",
    "* ['Python','_V','_NP']\n",
    "* ['Python','trains','_NP']\n",
    "* ['Python','trains','_A','_NP','_P','_A','_N']\n",
    "* ['Python','trains','logistic','_NP','_P','_A','_N']\n",
    "* ['Python','trains','logistic','_N','_P','_A','_N']\n",
    "* ['Python','trains','logistic','data science','_P','_A','_N']\n",
    "* ['Python','trains','logistic','data science','about','_A', '_N']\n",
    "* ['Python','trains','logistic','data science','about','logistic','_N']\n",
    "* ['Python','trains','logistic','data science','about','logistic','Python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(token):\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar, tokens):\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # skip over terminals\n",
    "        if is_terminal(token):\n",
    "            continue\n",
    "\n",
    "        # if we get here, we found a non-terminal token\n",
    "        # so we need to choose a replacement randomly\n",
    "\n",
    "        replacement = da.random.sample(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        \n",
    "        # call expand on the new list of tokens\n",
    "        return expand(grammar, tokens)\n",
    "        # if we get here we had all terminals and nothing more to do.\n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data science tests data science'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(generate_sentence(grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gibbs sampling\n",
    "\n",
    "def roll_dice():\n",
    "    return da.random.rand_index(6) + 1\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_dice()\n",
    "    d2 = roll_dice()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_dice()\n",
    "\n",
    "# other direction is more complicated\n",
    "# For example, if you know that y is 2, then necessarily x is 1 \n",
    "# since the only way two dice can sum to 2 is if both of them are 1\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # if total is 7 or less, the first dice equally likely to be\n",
    "        # 1, 2, 3, .. total - 1\n",
    "        return da.random.rand_index(y - 1) + 1\n",
    "    else:\n",
    "        return da.random.sample(range(y - 6, 7))\n",
    "        # y = 8 -> x >= 2\n",
    "        # y = 9 -> x >= 3\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_x_given_y(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2 # doesnt matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "# ???\n",
    "\n",
    "gibbs_sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.compare_distributions.<locals>.<lambda>()>,\n",
       "            {(3, 5): [1, 0],\n",
       "             (6, 12): [0, 1],\n",
       "             (3, 9): [1, 0],\n",
       "             (5, 10): [0, 1],\n",
       "             (4, 10): [1, 1],\n",
       "             (4, 6): [0, 3],\n",
       "             (5, 9): [1, 0],\n",
       "             (1, 6): [2, 1],\n",
       "             (6, 7): [0, 1],\n",
       "             (6, 10): [1, 0],\n",
       "             (1, 2): [1, 0],\n",
       "             (4, 5): [1, 1],\n",
       "             (1, 3): [0, 1],\n",
       "             (3, 8): [1, 0]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts\n",
    "compare_distributions(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.5932), (0, 0.2068), (1, 0.2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def sample_from(weights):\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * da.random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0: return i\n",
    "\n",
    "sample_array = [1, 1, 3]\n",
    "sample_size = 10000\n",
    "\n",
    "rands = Counter([sample_from(sample_array) for _ in range(sample_size)])\n",
    "probs = defaultdict(list)\n",
    "probs = [(y, x / sample_size) for y, x in rands.items()]\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_analysis_tools as da\n",
    "from collections import Counter, defaultdict\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "\n",
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each document\n",
    "document_lengths = list(map(len, documents))\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    For example, once we populate these, we can find, for example, the number of words in\n",
    "    documents[3] associated with topic 1 as\n",
    "\"\"\"\n",
    "document_topic_counts[3][1]\n",
    "\n",
    "\"\"\"\n",
    "    And we can find the number of times nlp is associated with topic 2 as:\n",
    "\"\"\"\n",
    "topic_word_counts[2][\"nlp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document d that are assigned to topic (plus some smoothing)\"\"\"\n",
    "    return (\n",
    "        (document_topic_counts[d][topic] + alpha) / (document_lengths[d] + K * alpha)\n",
    "    )\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # belirli bir topigin icerisindeki belirli word oranı\n",
    "    return (\n",
    "        (topic_word_counts[topic][word] + beta) / (topic_counts[topic] + W * beta)\n",
    "    )\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document, return the weight for the kth topic\"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.r.seed(0)\n",
    "document_topics = [[da.random.sample(range(K)) for word in document] for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d], document_topics[d])):\n",
    "\n",
    "            # remove this word/topic from the counts\n",
    "            # so that it doesnt influence the weights\n",
    "            if topic is None:\n",
    "                continue\n",
    "\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            if new_topic is None:\n",
    "                continue\n",
    "\n",
    "            # add +1 again\n",
    "            document_topic_counts[d][topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 2\n",
      "0 Python 2\n",
      "0 probability 2\n",
      "0 Hadoop 1\n",
      "0 Haskell 1\n",
      "0 Mahout 1\n",
      "0 Spark 1\n",
      "0 scikit-learn 1\n",
      "0 scipy 1\n",
      "0 programming languages 1\n",
      "0 libsvm 1\n",
      "1 C++ 2\n",
      "1 Big Data 2\n",
      "1 deep learning 2\n",
      "1 artificial intelligence 2\n",
      "1 mathematics 1\n",
      "1 theory 1\n",
      "2 machine learning 2\n",
      "2 regression 2\n",
      "2 statsmodels 2\n",
      "2 Postgres 1\n",
      "2 libsvm 1\n",
      "2 decision trees 1\n",
      "2 Python 1\n",
      "3 R 3\n",
      "3 neural networks 2\n",
      "3 statistics 2\n",
      "3 pandas 2\n",
      "3 Storm 1\n",
      "3 Cassandra 1\n",
      "3 scikit-learn 1\n",
      "3 databases 1\n",
      "3 HBase 1\n",
      "3 MySQL 1\n",
      "3 MongoDB 1\n",
      "3 support vector machines 1\n",
      "3 numpy 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0: print(k, word, count)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2675592439886072215c5492b56ef91d6259dc08377ceafc1fad216e79bf788"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}