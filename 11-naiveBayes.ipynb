{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021', '4', 'august', 'hello', 'is', 'today', 'world'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import data_analysis_tools as da\n",
    "def tokenize(message: str):\n",
    "    message = message.lower()\n",
    "    all_words = re.findall(\"[a-z0-9]+\", message)\n",
    "    return set(all_words)\n",
    "\n",
    "tokenize(\"Hello world! Today is August 4 2021.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Subject: re : indian springs\\nthis deal is to book the teco pvr revenue . it is my understanding that teco\\njust sends us a check , i haven ' t received an answer as to whether there is a\\npredermined price associated with this deal or if teco just lets us know what\\nwe are giving . i can continue to chase this deal down if you need .\", 'ham')\n",
      "---------------------------------------------------------------------\n",
      "{'us', 'understanding', 'know', 'whether', 'the', 'predermined', 'check', 'this', 'teco', 'revenue', 'subject', 'or', 'we', 're', 't', 'associated', 'down', 'springs', 'can', 'there', 'is', 'received', 'that', 'giving', 'you', 'i', 'indian', 'deal', 'lets', 'what', 'to', 'chase', 'haven', 'need', 'as', 'are', 'sends', 'it', 'my', 'an', 'book', 'answer', 'just', 'price', 'pvr', 'with', 'if', 'continue', 'a'}\n"
     ]
    }
   ],
   "source": [
    "data = da.file_ops.read_csv('./files/spam_ham_dataset.csv')\n",
    "data = [ (data_i['text'], data_i['label']) for data_i in data ]\n",
    "print(data[4])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(tokenize(data[4][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4136 test 1035\n",
      "ham: 2944, spam: 1192\n",
      "ham p= 0.7117988394584139, spam p= 0.28820116054158607\n"
     ]
    }
   ],
   "source": [
    "spam_count = 0\n",
    "ham_count = 0\n",
    "\n",
    "def spam_ham_count_from_data(data):\n",
    "    \"\"\"return spam_count, ham_count from given data\"\"\"\n",
    "    spam_count = ham_count = 0\n",
    "    for _, label in data:\n",
    "        if label == 'spam':\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            ham_count += 1\n",
    "\n",
    "    return spam_count, ham_count\n",
    "\n",
    "train, test = da.random.split_data(data, 0.2)\n",
    "print('train:',len(train), 'test',len(test))\n",
    "spam_count, ham_count = spam_ham_count_from_data(train)\n",
    "\n",
    "p_ham = ham_count / (spam_count + ham_count)\n",
    "p_spam = spam_count / (spam_count + ham_count)\n",
    "print(f'ham: {ham_count}, spam: {spam_count}')\n",
    "print(f'ham p= {p_ham}, spam p= {p_spam}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    \n",
    "    for message, is_spam in training_set:\n",
    "        tokenized = tokenize(message)\n",
    "        for word in tokenized:\n",
    "            counts[word][0 if is_spam == 'spam' else 1] += 1\n",
    "    return counts\n",
    "\n",
    "ham_spam_counts = count_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets w, p(w | spam) and p(w | ~spam) \"\"\"\n",
    "    return [(w,\n",
    "            (spam + k) / (total_spams + 2 * k),\n",
    "            (non_spam + k) / (total_non_spams + 2 * k))\n",
    "            for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    from math import log, exp\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_spam = log_prob_nospam = 0.0\n",
    "\n",
    "    for word, prob_spam, prob_nospam in word_probs:\n",
    "        if word in message_words:\n",
    "            log_prob_spam += log(prob_spam)\n",
    "            log_prob_nospam += log(prob_nospam)\n",
    "            # print(word, prob_spam, prob_nospam)\n",
    "        else:\n",
    "            log_prob_spam += log(1.0 - prob_spam)\n",
    "            log_prob_nospam += log(1.0 - prob_nospam)\n",
    "\n",
    "        prob_spam = exp(log_prob_spam)\n",
    "        prob_nospam = exp(log_prob_nospam)\n",
    "\n",
    "    # print(prob_spam, prob_nospam)\n",
    "    predicted = 0.0\n",
    "    try:\n",
    "        predicted = prob_spam / (prob_spam + prob_nospam)\n",
    "    except:\n",
    "        pass\n",
    "    return predicted\n",
    "\n",
    "word_probs = word_probabilities(ham_spam_counts, spam_count, ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: wed unify mtg 2 : 00 - 2 allen center , 12 th floor conf room\n",
      "attached is the presentation that jeff reviewed last week in the meeting .\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by brenda f herod / hou / ect on 06 / 27 / 2000\n",
      "08 : 39 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "jeff johnson @ enron\n",
      "06 / 19 / 2000 05 : 34 pm\n",
      "to : beth perlman / hou / ect @ ect , bryce baxter / hou / ect @ ect , brent a\n",
      "price / hou / ect @ ect , leslie reeves / hou / ect @ ect , laura e scott / cal / ect @ ect ,\n",
      "sheila glover / hou / ect @ ect , jefferson d sorenson / hou / ect @ ect , robert\n",
      "superty / hou / ect @ ect , brenda f herod / hou / ect @ ect , kenneth m\n",
      "harmon / hou / ect @ ect , dave nommensen / hou / ect @ ect , regan m smith / hou / ect @ ect ,\n",
      "laray . odum @ luminant . com , nadeem . abbasi @ luminant . com , scott\n",
      "williamson / hou / ect @ ect , paul f poellinger / hou / ect @ ect , john\n",
      "simmons / na / enron @ enron , chris hanz / corp / enron @ enron , sally beck / hou / ect @ ect ,\n",
      "rebecca ford / hou / ect @ ect\n",
      "cc : angelina v lorio / hou / ect @ ect , john paskin / lon / ect @ ect\n",
      "subject : wed unify mtg 2 : 00 - 2 allen center , 12 th floor conf room\n",
      "please note that the wed meeting will be held as planned from 2 : 00 - 4 : 00 pm in\n",
      "the 2 allen center in the 12 th floor forum conference room .\n",
      "the room entrance is just off the elevator bank foyer .\n",
      "here is the draft of the presentation that we will discuss on wednesday . we\n",
      "are interested in your input and feedback in the meeting on wednesday .\n",
      "thanks .\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by jeff johnson / corp / enron on 06 / 19 / 2000\n",
      "05 : 20 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "alarm enabled : an alarm will occur on 06 / 21 / 2000 01 : 55 pm\n",
      "calendar entry\n",
      "brief description :\n",
      "date :\n",
      "time :\n",
      "unify system - - problems and proposed immediate projects . two allen center ,\n",
      "12 th floor forum\n",
      "06 / 21 / 2000\n",
      "02 : 00 pm - 04 : 00 pm\n",
      "detailed description :\n",
      "the meeting will be held in two allen center on the 12 th floor in the large\n",
      "conference room ( called the forum ) . the room entrance is just off the\n",
      "elevator bank in the foyer .\n",
      "we believe that we have an approach to immediately begin to address key\n",
      "performance issues in the unify system .\n",
      "to be successful with the proposed projects , we need your input , sponsorship\n",
      "and participation . angie lorio will find a location and will publish as soon\n",
      "as possible .\n",
      "the proposed agenda is as follows :\n",
      "i . unify - - current state , strengths , issues\n",
      "ii . conceptual overview of proposed solutions\n",
      "organization building\n",
      "performance monitoring team\n",
      "sybase 12 upgrade\n",
      "sql server testing\n",
      "middle tier rearchitecture project\n",
      "scoping for a re - write effort\n",
      "iii . solutions proposals / plans for immediate review / kickoff\n",
      "proposed scope\n",
      "assumptions\n",
      "high timelines / cost estimates\n",
      "staffing\n",
      "iv . facilitated discussion on unify\n",
      "future growth / changes required for unify\n",
      "new project prioritization\n",
      "invitations have been sent to : beth perlman / hou / ect @ ect , bryce\n",
      "baxter / hou / ect @ ect , brent a price / hou / ect @ ect , leslie reeves / hou / ect @ ect ,\n",
      "laura e scott / cal / ect @ ect , sheila glover / hou / ect @ ect , jefferson d\n",
      "sorenson / hou / ect @ ect , robert superty / hou / ect @ ect , brenda f herod / hou / ect @ ect ,\n",
      "kenneth m harmon / hou / ect @ ect , dave nommensen / hou / ect @ ect , regan m\n",
      "smith / hou / ect @ ect , laray . odum @ luminant . com , nadeem . abbasi @ luminant . com , scott\n",
      "williamson / hou / ect @ ect , paul f poellinger / hou / ect @ ect\n",
      "optional invitees : chris hanz / corp / enron @ enron , john simmons / na / enron @ enron ,\n",
      "angie lorio\n",
      "this meeting repeats starting on ( if the date occurs on a weekend the\n",
      "meeting ) .\n",
      "ham \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 1\n",
    "message = test[test_index][0]\n",
    "label = test[test_index][1]\n",
    "print(f'{message}\\n{label} ')\n",
    "spam_probability(word_probs, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: ham, predicted spam prob: 4.074643458321017e-34\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 1.3309044680619976e-68\n",
      "actual: ham, predicted spam prob: 3.0118928083352554e-32\n",
      "actual: ham, predicted spam prob: 3.805504854071566e-25\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 1.9254460668151395e-61\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: spam, predicted spam prob: 1.0\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: spam, predicted spam prob: 4.2040825553278734e-14\n",
      "actual: ham, predicted spam prob: 4.0926219152236524e-22\n",
      "actual: ham, predicted spam prob: 8.674727768735842e-38\n",
      "actual: spam, predicted spam prob: 1.9302380206633338e-10\n",
      "actual: spam, predicted spam prob: 7.300563740446168e-11\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 4.550775546608689e-35\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 2.482139125519755e-23\n",
      "actual: ham, predicted spam prob: 2.648495674547708e-38\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 1.5725613794935187e-34\n",
      "actual: spam, predicted spam prob: 9.543462016952359e-10\n",
      "actual: ham, predicted spam prob: 2.9558835190541795e-27\n",
      "actual: ham, predicted spam prob: 6.585560078944523e-38\n",
      "actual: spam, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 1.3735469571554232e-33\n",
      "actual: ham, predicted spam prob: 4.659712301949215e-44\n",
      "actual: ham, predicted spam prob: 4.909979019047923e-64\n",
      "actual: ham, predicted spam prob: 4.810327648393613e-28\n",
      "actual: spam, predicted spam prob: 1.0\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: spam, predicted spam prob: 0.968128328832901\n",
      "actual: spam, predicted spam prob: 1.0\n",
      "actual: ham, predicted spam prob: 2.281580008478362e-33\n",
      "actual: spam, predicted spam prob: 1.0451313046547913e-09\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 3.745772931846987e-32\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: spam, predicted spam prob: 0.0\n",
      "actual: spam, predicted spam prob: 1.0\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 0.01679365918672354\n",
      "actual: ham, predicted spam prob: 7.150116059371507e-39\n",
      "actual: spam, predicted spam prob: 1.0\n",
      "actual: ham, predicted spam prob: 2.1611646860206832e-45\n",
      "actual: ham, predicted spam prob: 8.119326447949769e-40\n",
      "actual: ham, predicted spam prob: 1.518659074165251e-35\n"
     ]
    }
   ],
   "source": [
    "for test_index in range(50):\n",
    "    message = test[test_index][0]\n",
    "    label = test[test_index][1]\n",
    "    print(f'actual: {label}, predicted spam prob: {spam_probability(word_probs, message)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = da.ml.NaiveBayesClassifier(k=0.01)\n",
    "classifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: spam, predicted spam prob: 1.0\n",
      "actual: spam, predicted spam prob: 4.3062193849647404e-13\n",
      "actual: ham, predicted spam prob: 8.0099593110892e-40\n",
      "actual: ham, predicted spam prob: 1.3126400865957259e-80\n",
      "actual: ham, predicted spam prob: 0.0\n",
      "actual: ham, predicted spam prob: 1.860959326964966e-103\n",
      "actual: ham, predicted spam prob: 1.6594643156190965e-48\n",
      "actual: ham, predicted spam prob: 1.2524650911525044e-20\n",
      "actual: ham, predicted spam prob: 3.467234431841853e-99\n",
      "actual: ham, predicted spam prob: 1.2459195050807694e-63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for test_index in range(60, 70):\n",
    "    message = test[test_index][0]\n",
    "    label = test[test_index][1]\n",
    "    print(f'actual: {label}, predicted spam prob: {classifier.classify(message)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.150060847152339e-12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(\"I am new to this company, so because of that i need your assistance sir!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.882828309290226e-08"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(\"Free viagra, click now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.8753623188405797\n",
      "precision score: 0.9587628865979382\n",
      "recall score: 0.6058631921824105\n",
      "f1 score: 0.7425149700598803\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "tp, fp, tn, fn = classifier.test(test, 0.7)\n",
    "print(f'accuracy score: {da.ml.accuracy(tp, fp, fn, tn)}')\n",
    "print(f'precision score: {da.ml.precision(tp, fp, fn, tn)}')\n",
    "print(f'recall score: {da.ml.recall(tp, fp, fn, tn)}')\n",
    "print(f'f1 score: {da.ml.f1_score(tp, fp, fn, tn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9091787439613527\n",
      "precision score: 0.941908713692946\n",
      "recall score: 0.739413680781759\n",
      "f1 score: 0.8284671532846715\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = classifier.test(test, 0.000000001)\n",
    "print(f'accuracy score: {da.ml.accuracy(tp, fp, fn, tn)}')\n",
    "print(f'precision score: {da.ml.precision(tp, fp, fn, tn)}')\n",
    "print(f'recall score: {da.ml.recall(tp, fp, fn, tn)}')\n",
    "print(f'f1 score: {da.ml.f1_score(tp, fp, fn, tn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spammiest words: [('cheap', 0.04111508196171205, 3.3967160549181054e-06), ('xp', 0.042792906159292626, 3.3967160549181054e-06), ('biz', 0.046987466653244075, 3.3967160549181054e-06), ('cialis', 0.046987466653244075, 3.3967160549181054e-06), ('drugs', 0.04782637875203436, 3.3967160549181054e-06), ('paliourg', 0.052859851344776096, 3.3967160549181054e-06), ('php', 0.05621549973993725, 3.3967160549181054e-06), ('viagra', 0.05957114813509841, 3.3967160549181054e-06), ('meds', 0.06292679653025957, 3.3967160549181054e-06), ('prescription', 0.06628244492542072, 3.3967160549181054e-06)]\n",
      "\n",
      "hammiest words: [('enron', 8.389120987902887e-06, 0.3994572047744241), ('hpl', 8.389120987902887e-06, 0.30536817005319256), ('daren', 8.389120987902887e-06, 0.2768357551918805), ('meter', 8.389120987902887e-06, 0.209580777304502), ('nom', 8.389120987902887e-06, 0.15693167845327138), ('mmbtu', 8.389120987902887e-06, 0.14198612781163172), ('xls', 8.389120987902887e-06, 0.13960842657318903), ('volumes', 8.389120987902887e-06, 0.11990747345466403), ('sitara', 8.389120987902887e-06, 0.11039666850089333), ('nomination', 8.389120987902887e-06, 0.09035604377687652)]\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"p(spam | message contains word)\"\"\"\n",
    "    _, prob_spam, prob_nospam = word_prob\n",
    "    return prob_spam / (prob_spam + prob_nospam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "spammiest_words = words[-10:]\n",
    "hammiest_words = words[:10]\n",
    "\n",
    "print(f'spammiest words: {spammiest_words}\\n')\n",
    "print(f'hammiest words: {hammiest_words}')\n",
    "\n",
    "\"\"\"\n",
    "    Improvement ideas: \n",
    "\n",
    "    1- This classifier takes into account every word that appears in the training set, even words\n",
    "    that appear only once. Modify the classifier to accept an optional min_count threshhold\n",
    "    and ignore tokens that don’t appear at least that many times\n",
    "\n",
    "    2- Use a stemmer algorithm for words. https://tartarus.org/martin/PorterStemmer/index-old.html\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2675592439886072215c5492b56ef91d6259dc08377ceafc1fad216e79bf788"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}