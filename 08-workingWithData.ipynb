{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bucketize(point, bucket_size):\n",
    "    return bucket_size * np.floor(point / bucket_size)\n",
    "\n",
    "def make_hist(points, bucket_size):\n",
    "    return Counter([bucketize(point, bucket_size) for point in points])\n",
    "\n",
    "def plot_hist(points, bucket_size, title='Default Title'):\n",
    "    bucketed_points = make_hist(points, bucket_size)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(title)\n",
    "    plt.bar(bucketed_points.keys(), bucketed_points.values())\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\" points = [24, 26, 33, 37, 12, 27, 39, 44, 31, 32, 33, 9, 1]\n",
    "\n",
    "bucketed_points = make_hist(points, bucket_size=10)\n",
    "\n",
    "plot_hist(bucketed_points) \"\"\"\n",
    "\n",
    "uniform = [np.random.uniform() * 200 - 100 for _ in range(10000)]\n",
    "\n",
    "normal = np.random.normal(0, 57, 10000)\n",
    "\n",
    "plot_hist(uniform, 10, 'Uniform Histogram')\n",
    "\n",
    "plot_hist(normal, 10, 'Normal Histogram')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.normal(0, 20, 1000)\n",
    "\n",
    "y_spread = 10\n",
    "\n",
    "ys1 = np.random.normal(0, y_spread, 1000)\n",
    "ys1 = [x + y for x, y in zip(xs, ys1)]\n",
    "\n",
    "\n",
    "ys2 = np.random.normal(0, y_spread, 1000)\n",
    "ys2 = [-x + y for x, y in zip(xs, ys2)]\n",
    "\n",
    "# plot_hist(xs, 1, 'XS')\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.scatter(xs, ys1, marker='.', color='blue', label='ys1')\n",
    "plt.scatter(xs, ys2, marker='.', color='green', label='ys2')\n",
    "\n",
    "plt.xlabel('xs')\n",
    "plt.ylabel('ys')\n",
    "\n",
    "plt.legend(loc=9)\n",
    "plt.show()\n",
    "\n",
    "print('Correlation between xs and ys1:', np.corrcoef(xs, ys1)[1][0])\n",
    "print('Correlation between xs and ys2:', np.corrcoef(xs, ys2)[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many dimensions\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['xs'] = xs\n",
    "df['ys1'] = ys1\n",
    "df['ys2'] = ys2\n",
    "\n",
    "sns.pairplot(df, height=3.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and munging\n",
    "\"\"\" def parse_row(input_row, parsers):\n",
    "    return [parser(value) if parser is not None else value for value, parser in zip(input_row, parsers)] \"\"\"\n",
    "\n",
    "def parse_row(input_row, parsers):\n",
    "    return [try_or_none(parser, value) if parser is not None else value\n",
    "        for value, parser in zip(input_row, parsers)]\n",
    "\n",
    "def parse_rows_with(reader, parsers):\n",
    "    for row in reader:\n",
    "        yield parse_row(row, parsers)\n",
    "\n",
    "def try_or_none(f, x):\n",
    "    try: return f(x)\n",
    "    except: return None\n",
    "\n",
    "\n",
    "import dateutil.parser\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('./files/comma_sep_stock_prices.csv', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in parse_rows_with(reader, [dateutil.parser.parse, None, float]):\n",
    "        data.append(line)\n",
    "\n",
    "print(data)\n",
    "\n",
    "for row in data:\n",
    "    if any(x is None for x in row):\n",
    "        print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse_field(field_name, value, parser_dict):\n",
    "    parser = parser_dict.get(field_name)\n",
    "    if parser is not None:\n",
    "        return try_or_none(parser, value)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def parse_dict(input_dict, parser_dict):\n",
    "    return {field_name: try_parse_field(field_name, value, parser_dict) for field_name, value in input_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulating data\n",
    "\n",
    "# get from a file\n",
    "data = []\n",
    "\n",
    "with open('./files/stocks.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "max_appl_price = max(row[\"Close\"] for row in data if row[\"Symbol\"] == \"AAPL\")\n",
    "\n",
    "# group rows by symbol\n",
    "by_symbol = defaultdict(list)\n",
    "for row in data:\n",
    "    by_symbol[row[\"Symbol\"]].append(row)\n",
    "\n",
    "print('By symbol keys:', by_symbol.keys())\n",
    "\n",
    "# use a dict comprehension to find the max for each symbol\n",
    "max_price_by_symbol = {symbol: max(row[\"Close\"] for row in grouped_rows) for symbol, grouped_rows in by_symbol.items()}\n",
    "print('Max of each symbol:', max_price_by_symbol)\n",
    "\n",
    "def picker(field_name):\n",
    "    return lambda row: row[field_name]\n",
    "\n",
    "def pluck(field_name, rows):\n",
    "    return map(picker(field_name), rows)\n",
    "\n",
    "def group_by(grouper, rows, value_transform=None):\n",
    "    grouped = defaultdict(list)\n",
    "    for row in rows:\n",
    "        grouped[grouper(row)].append(row)\n",
    "    if value_transform == None:\n",
    "        return grouped\n",
    "    else:\n",
    "        return { key: value_transform(rows) for key, rows in grouped.items() }\n",
    "\n",
    "max_price_by_symbol = group_by(picker(\"Symbol\"), data, lambda rows: max(pluck(\"Close\", rows)))\n",
    "max_price_by_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_price_change(yesterday, today):\n",
    "    return float(today[\"Close\"]) / float(yesterday[\"Close\"]) - 1\n",
    "\n",
    "def day_over_day_changes(grouped_rows):\n",
    "    ordered = sorted(grouped_rows, key=picker(\"Date\"))\n",
    "    return [{ \"symbol\": today[\"Symbol\"],\n",
    "    \"date\": today[\"Date\"],\n",
    "    \"change\": percent_price_change(yesterday, today) } \n",
    "    for yesterday, today in zip(ordered, ordered[1:])]\n",
    "\n",
    "changes_by_symbol = group_by(picker(\"Symbol\"), data, day_over_day_changes)\n",
    "changes_by_symbol\n",
    "\n",
    "all_changes = [change for changes in changes_by_symbol.values()\n",
    "    for change in changes]\n",
    "\n",
    "# print(all_changes)\n",
    "\n",
    "print('max:', max(all_changes, key=picker(\"change\")))\n",
    "print('min:', min(all_changes, key=picker(\"change\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling\n",
    "import data_analysis_tools as da\n",
    "\n",
    "a_to_b = da.euclidean([63, 150], [67, 160])\n",
    "a_to_c = da.euclidean([63, 150], [70, 171])\n",
    "b_to_c = da.euclidean([67, 160], [70, 171])\n",
    "\n",
    "print('Inches', a_to_b, a_to_c, b_to_c)\n",
    "\n",
    "a_to_b = da.euclidean([160, 150], [170.2, 160]) # 14.28\n",
    "a_to_c = da.euclidean([160, 150], [177.8, 171]) # 27.53\n",
    "b_to_c = da.euclidean([170.2, 160], [177.8, 171]) # 13.37\n",
    "\n",
    "\n",
    "\n",
    "print('Centimeters', a_to_b, a_to_c, b_to_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data_matrix):\n",
    "    _, num_cols = data_matrix.shape\n",
    "    means = [da.mean(da.get_column(data_matrix, j)) for j in range(num_cols)]\n",
    "\n",
    "    stdevs = [da.standard_deviation(da.get_column(data_matrix, j)) for j in range(num_cols)]\n",
    "\n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data_matrix):\n",
    "    means, stdevs = scale(data_matrix)\n",
    "\n",
    "    def rescaled(i, j):\n",
    "        if stdevs[j] > 0:\n",
    "            return (data_matrix[i][j] - means[j]) / stdevs[j]\n",
    "        else:\n",
    "            return data_matrix[i][j]\n",
    "\n",
    "    num_rows, num_cols = data_matrix.shape\n",
    "    return da.make_matrix(num_rows, num_cols, rescaled)\n",
    "\n",
    "example_matrix = [[19, 2, 3, 2, 17], \n",
    "                  [19, 2, 3, 5, 2], \n",
    "                  [19, -2, 3, 5, -2]]\n",
    "\n",
    "\n",
    "rescale(np.array(example_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction\n",
    "\n",
    "def de_mean_matrix(A):\n",
    "    nr, nc = A.shape\n",
    "    column_means, _ = scale(A)\n",
    "\n",
    "    def entry_fn(i, j):\n",
    "        return A[i][j] - column_means[j]\n",
    "\n",
    "    return da.make_matrix(nr, nc, entry_fn=entry_fn)\n",
    "\n",
    "xs = da.random.normal(0, 10, 15)\n",
    "ys = list(xi / 2 + int(da.random.normal(0, 4, 1)) for xi in xs)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()\n",
    "\n",
    "print(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_analysis_tools as da\n",
    "from functools import partial\n",
    "\n",
    "vector = [3, 6, -4, -9]\n",
    "\n",
    "def direction(w):\n",
    "    mag = da.magnitude(w)\n",
    "    return [wi / mag for wi in w]\n",
    "\n",
    "def directional_variance_i(xi, w):\n",
    "    return da.dot_product(xi, direction(w))\n",
    "\n",
    "def directional_variance(x, w):\n",
    "    return sum(directional_variance_i(xi, w) for xi in x)\n",
    "\n",
    "print(f'direction of vector={vector} is {direction(vector)}')\n",
    "\n",
    "def directional_variance_gradient_i(xi, w):\n",
    "    projection_length = da.dot_product(xi, direction(w))\n",
    "    return [2 * projection_length * xij for xij in xi]\n",
    "\n",
    "def directional_variance_gradient(x, w):\n",
    "    return da.vector_sum(directional_variance_gradient_i(xi, w) for xi in x)\n",
    "\n",
    "\n",
    "def first_principal_component(x):\n",
    "    guess = [1 for _ in x[0]]\n",
    "    unscaled_maximizer = da.maximize_batch(partial(directional_variance, x),\n",
    "                                                            partial(directional_variance_gradient, x),\n",
    "                                                            guess)\n",
    "    return direction(unscaled_maximizer)\n",
    "\n",
    "\n",
    "def first_principal_component_sgd(x):\n",
    "    guess = [1 for _ in x[0]]\n",
    "    unscaled_maximized = da.maximize_stochastic(\n",
    "        lambda x, _, w: directional_variance_i(x, w),\n",
    "        lambda x, _, w: directional_variance_gradient_i(x, w),\n",
    "        x,\n",
    "        [0 for _ in x],\n",
    "        guess)\n",
    "    return direction(unscaled_maximized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = first_principal_component(list(zip(xs, ys)))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_principal_component_sgd(list(zip(xs, ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(v, w):\n",
    "    projection_length = da.dot_product(v, w)\n",
    "    return da.scalar_multiply(projection_length, w)\n",
    "\n",
    "def remove_projection_from_vector(v, w):\n",
    "    return da.vector_substract(v, project(v, w))\n",
    "\n",
    "def remove_projection(x, w):\n",
    "    return [remove_projection_from_vector(xi, w) for xi in x]\n",
    "\n",
    "\n",
    "def principal_component_analysis(x, num_components):\n",
    "    components = []\n",
    "    for _ in range(num_components):\n",
    "        component = first_principal_component(x)\n",
    "        components.append(component)\n",
    "        x = remove_projection(x, component)\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = principal_component_analysis(list(zip(xs, ys)), 4)\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = da.principal_component_analysis(list(zip(xs, ys)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2675592439886072215c5492b56ef91d6259dc08377ceafc1fad216e79bf788"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}